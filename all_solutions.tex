\protect \hypertarget {soln:1.1}{}
\begin{solution}{{1.1}}
\end{solution}
\protect \hypertarget {soln:1.2}{}
\begin{solution}{{1.2}}
Ответы: $0$, $0$, $0$, $\sum x_i^2$.
\end{solution}
\protect \hypertarget {soln:1.3}{}
\begin{solution}{{1.3}}
\begin{enumerate}
\item \(\htheta = \sum \left((y_i - z_i)(x_i - z_i) \right) / \sum \left(x_i - z_i\right)^2 \)
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:1.4}{}
\begin{solution}{{1.4}}
\(\hat{\alpha} = 0, \ \hb = 1 \)
\end{solution}
\protect \hypertarget {soln:1.5}{}
\begin{solution}{{1.5}}
 % 1.5.
Рассмотрим регрессию суммы $(y_i + z_i)$ на саму себя. Естественно, в ней
\[
\widehat{y_i + z_i} = 0 + 1 \cdot (y_i + z_i).
\]

Отсюда получаем, что $\hat{\alpha} + \hat{\gamma} = 0$ и $\hb + \hat{\delta} = 1$.
\end{solution}
\protect \hypertarget {soln:1.6}{}
\begin{solution}{{1.6}}

Исходя из условия, нужно оценить методом МНК коэффициенты двух следующих моделей:
\[y_i = \alpha + \beta x_i + \e_i \]
\[y_i = \frac{\gamma}{2} + \frac{\delta}{2} x_i + \frac{1}{2} v_i \]

Заметим, что на минимизацию суммы квадратов остатков коэффициент \(1/2\) не влияет, следовательно:
\[\hat{\gamma} = 2\hat{\alpha}, \ \hat{\delta} = 2 \hb  \]

\end{solution}
\protect \hypertarget {soln:1.7}{}
\begin{solution}{{1.7}}
Выпишем задачу:
\[
\begin{cases}
RSS = \sum\limits_{i=1}^{n}(y_i - \hb_1x_i - \hb_2z_i)^2 \rightarrow \min\limits_{\hb_1, \hb_2}\\
\hb_1 + \hb_2 = 1
\end{cases}
\]

Можем превратить ее в задачу минимизации функции одного аргумента:
\[
RSS =  \sum\limits_{i=1}^{n}(y_i - x_i - \hb_2(z_i-x_i))^2 \rightarrow \min_{\hb_2}
\]

Выпишем условия первого порядка:
\[
\frac{\partial RSS}{\partial \hb_2} = \sum\limits_{i=1}^{n}2(y_i-x_i-\hb_2(z_i-x_i))(x_i-z_i)=0
\]

Отсюда:
\[
\sum\limits_{i=1}^{n}(y_i-x_i)(x_i-z_i) + \hb_2\sum\limits_{i=1}^{n}(z_i-x_i)^2 = 0 \Rightarrow \hb_2 = \frac{\sum\limits_{i=1}^n (y_i-x_i)(z_i-x_i)}{\sum\limits_{i=1}^n (z_i-x_i)^2}
\]

А $\hb_1$ найдется из соотношения $\hb_1+\hb_2 = 1$.

\end{solution}
\protect \hypertarget {soln:1.8}{}
\begin{solution}{{1.8}}
Обозначив вес первого слитка за \(\beta_1\), вес второго слитка за \(\beta_2\), а показания весов за \(y_i\), получим, что
\[y_1 = \beta_1 + \e_1, \ y_2 = \beta_2 + \e_2, \ y_3 = \beta_1 + \beta_2 + \e_3\]

Тогда
\[(300 - \beta_1)^2 + (200 - \beta_2)^2 + (400 - \beta_1 - \beta_2)^2 \rightarrow \min \limits_{\beta_1,\  \beta_2} \]
\[\hb_1 = \frac{800}{3}, \ \hb_2 = \frac{500}{3} \]
\end{solution}
\protect \hypertarget {soln:1.9}{}
\begin{solution}{{1.9}}
Можем воспользоваться готовой формулой для регрессии на константу:
\[
\hb = \bar{y} = \frac{10+10+3}{3} = \frac{23}{3}
\]

(можно решить задачу $2(10-\beta)^2 + (3-\beta)^2\rightarrow \min$)

\end{solution}
\protect \hypertarget {soln:1.10}{}
\begin{solution}{{1.10}}
\end{solution}
\protect \hypertarget {soln:1.11}{}
\begin{solution}{{1.11}}
\end{solution}
\protect \hypertarget {soln:1.12}{}
\begin{solution}{{1.12}}
\end{solution}
\protect \hypertarget {soln:1.13}{}
\begin{solution}{{1.13}}
Пусть \(\bar{y}\) — средний \(y\) до добавления нового наблюдения, \(\bar{y}'\) — после добавления нового наблюдения. Будем считать, что изначально было \(n\) наблюдений. Заметим, что
\[\bar{y}' = \frac{(y_1 + \ldots + y_n) + y_{n+1}}{n + 1} = \frac{n \bar{y} + y_{n + 1}}{n + 1} = \frac{n}{n+ 1}\bar{y} + \frac{1}{n+1}y_{n+1}\]

Покажем, что \(TSS\) может только увеличится при добавлении нового наблюдения (остается неизменным при \(y_{n+1} = \bar{y}\)):
\[TSS'= \sum_{i = 1}^{n + 1} (y_i - \bar{y}')^2 = \sum_{i = 1}^{n} (y_i - \bar{y} + \bar{y} - \bar{y}')^2 + (y_{n + 1} - \bar{y}')^2 = \]
\[=\sum_{i = 1}^{n} (y_i - \bar{y})^2 + n(\bar{y} - \bar{y}')^2 + (y_{n + 1} - \bar{y}')^2  = TSS + \frac{n}{n+1} (y_{n+1} - \bar{y})^2\]

Следовательно, \(TSS' \geqslant TSS\).

Также сумма \(RSS\) может только вырасти или остаться постоянной при добавлении нового наблюдения. Действительно, новое $(n+1)$-ое слагаемое в сумме неотрицательно. А сумма $n$ слагаемых минимальна при старых коэффициентах, а не при новых.

\(ESS\) и \(R^2\) могут меняться в обе стороны. Например, рассмотрим ситуацию, где точки лежат симметрично относительно некоторой горизонтальной прямой. При этом $ESS=0$. Добавим наблюдение — $ESS$ вырастет, удалим наблюдение — $ESS$ вырастет.
\end{solution}
\protect \hypertarget {soln:1.14}{}
\begin{solution}{{1.14}}
\begin{enumerate}
\item $R^2$ упал до нуля.
\item Да, можно. Если добавить точку далеко слева внизу от исходного набора данных, то наклон линии регрессии будет положительный. Если далеко справа внизу, то отрицательный. Будем двигать точку так, чтобы поймать нулевой наклон прямой. Получим $ESS=0$.
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:2.15}{}
\begin{solution}{{2.15}}
\end{solution}
\protect \hypertarget {soln:2.16}{}
\begin{solution}{{2.16}}
\end{solution}
\protect \hypertarget {soln:2.17}{}
\begin{solution}{{2.17}}
\end{solution}
\protect \hypertarget {soln:2.18}{}
\begin{solution}{{2.18}}
\end{solution}
\protect \hypertarget {soln:3.19}{}
\begin{solution}{{3.19}}
\end{solution}
\protect \hypertarget {soln:3.20}{}
\begin{solution}{{3.20}}
Сферы с центром в начале координат. Проекция имеет хи-квадрат распределение с тремя степенями свободы.
\end{solution}
